# Background you will need for PGM
## Probability
### Expectation and Conditional Expectation
> For discrete variables
$$
E(X) = \sum_{x\in X}x \cdot p(x)
$$
For continuous variables
$$
E(X) = \int_{x}x \cdot p(x) dx
$$
Conditional Expextation
$$
E(X|y) = \int_x x\cdot p(x|y)dx
$$

### Variance, Covariance, Covariance Matrix and Correlation
> The variance of a RV X    
$$
\begin{aligned}
\delta_X^2=Var(X) &= \int_x[x-E(X)]^2\cdot p(x)dx
\\
&=E(X^2)-E^2(X)
\end{aligned}
$$
Conditional variance
$$
\begin{aligned}
\delta_{X|y}^2 = Var(X|y)&=\int_x[x-E(X|y)]^2\cdot p(x|y)dx
\\
&=E(X^2|y)-E^2(X|y)
\end{aligned}
$$
Covariance of RV X and Y
$$
\begin{aligned}
\sigma_{XY}^2&=E[(x-E(X))\cdot (y-E(Y))]\\
&=E(XY)-E(X)E(Y)
\end{aligned}
$$
Covariance Matrix for random vector $X^{N\times1}$
$$
\begin{aligned}
\Sigma_X^{N\times N}&=E[(x-E(X))(x-E(X))^T]\\&=E(XX^T)-E(X)E^T(X)
\end{aligned}
$$
The diagonal is variance and the off diagonal is covariance.    
Correlation between X and Y   
$$
\rho_{XY} = \frac{\sigma^2_{XY}}{\sigma_X\cdot\sigma_Y}\, -1\leq\rho_{XY}\leq1
$$

### Probability Rules
> Product Rule
$$
p(X,Y)=p(X|Y)p(Y)
$$
Chain Rule
$$
p(A,B,C)=p(A)p(B|A)p(C|B,A)
$$
Conditional Chain Rule
$$
p(A,B,C|D,E)=p(A|D,E)p(B|A,D,E)p(c|A,B,D,E)
$$
This run can be used for sequential data.
Union Rule
$$
p(X\cup Y) = p(x)+p(y)-p(X,Y)
$$
if $X$ and $Y$ are mutually exclusive
$$
p(X\cup Y) = p(x)+p(y)
$$
Sum Rule
$$
p(X)=\sum_Y p(X,Y)
$$     

__Bayes' Rule__
> $$
p(X|Y) = \frac{p(X)p(Y|X)}{p(Y)}
$$
where $p(X|Y)$ is called __posterior probability__ of $X$, $p(X)$ is called __prior probability__ of $X$, $p(Y|X)$ is called __likelihood__ of $X$ and $p(Y)$ is caled the probability of evidence.    
This can be further written as 
$$
p(H|E_1,E_2) = \frac{p(H|E_1)p(E_2|H,E_1)}{p(E_2|E_1)}=\frac{p(H|E_1)p(E_2|H,E_1)}{\sum_H p(E_2|H,E_1)p(H|E_1)}
$$
==if $E_1,E_2$ are conditionally independent given $H$, we have==
$$
p(H|E_1,E_2) = \frac{p(H|E_1)p(E_2|H)}{\sum_H p(E_2|H)p(H|E_1)}
$$
### Probability Distributions
- Distribution
    - Discrete
        - Categorical
            - Categorical Probability Distribution
            - Bernoulli Dsitribution for 2 class RV
            - Uniform Distribution
        - Integer
            - Binominal Distribution
            - Poisson Distribution
            - Multinominal Distribution
    - Continuous
        - Gaussian Distribution
        - Exponential Distribution
        - Beta Distribution (Probability Distribution)
        - Dirichlet Distribution (Probability vector Distribution)

__Categorical Distributions__
> Categorical distribution for a discrete random variable $X\in\{c_1,c_2,\dots,c_K\}$ with $K$ categorical values, denoted as
$$
X\sim Cat(k|\theta,K)
$$

>Bernoulli dsitribution for 2 class random variable $X\in\{0,1\}$
$$
X\sim Ber(x|\theta)
$$
where $P(X=1)=\theta,\,P(X=0)=1-\theta$, and 
$$
p(x) = \theta^x(1-\theta)^{1-x}
$$

>Uniform distribution $X\in\{c_1,c_2,\dots,c_K\}$
$$
X\sim Uniform(x|K)
$$
where $P(X=k)=\frac{1}{K}$

__Integer Distributions__
>Binominal dsitribution
Let Y be an integer random variabl $Y\sim Bin(y|N,\theta)$
$$
P(Y=y|N,\theta) = \binom{N}{y}\theta^y(1-\theta)^{N-y}
$$

>Poissin distribution for an integerrandom variable $X\in N$
$$
\begin{aligned}
X\sim Poisson(x|\theta)\\
P(X = x|\theta) = e^{-\lambda}\frac{\lambda^x}{x!}
\end{aligned}
$$

>Multinominal dsitribution
Let $\bold{Y}=(Y_1,Y_2,\dots,Y_K)$ be a random vector with K integer values
$$
\begin{aligned}
\bold Y&\sim Mult(y_1,y_2,\dots,y_K|N,\theta_1,\theta_2,\dots\theta_K)\\
P(\bold Y&=(y_1,y_2,\dots,y_K))=\frac{N!}{y_1!y_2!\dots y_K!}\theta_1^{y_1}\theta_2^{y_2}\dots \theta_K^{y_K}
\end{aligned}
$$
This distribution can be generated by repeat a catogorical RV $X\in\{c_1,c_2\dots c_K\}$ with $P(X=c_k)=\theta_k$, $N$ times, where $Y_k$ represents the number of times $X=c_k$ out of $N$ trials.

__Continuous Distributions__
>Gaussian dsitribution 
Unary continuous RV:
$$
p(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma}}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)
$$
Continuous random vectors
$$
p(\bold x|\bold{\mu},\Sigma) = \frac{1}{\sqrt{(2\pi)^K|\Sigma|}}\exp(-\frac{1}{2}(\bold x-\bold{\mu})^T\Sigma^{-1}(\bold x-\bold{\mu}))
$$

>Exponential distribution $X\sim Exp(x|\lambda)$
$$
p(x|\lambda)=\lambda e^{-\lambda x},\,x>0
$$

__About the relationship between Binominal, Poisson and Exponential distribution__
>Let's say that we have a time interval $[0,T)$, and we can divide it into $N$ segments, and for the $i$th segment $[\frac{(i-1)T}{N},\frac{iT}{N})$ the probability of $E$ happens is proportional to the length of the interval, and can be written as $\frac{\lambda T}{N}$. And we can see that if $N$ goes to infinity, this probability will goes to $0$, additionally we assume the interval is too short that $E$ will only happen once at most, and $E$ in different interval is independent. So the probability of $E$ happening $n$ times within $[0,T)$ will be
$$
p(n|N,\frac{\lambda T}{N}) = \binom{N}{n}(\frac{\lambda T}{N})^n(1-\frac{\lambda T}{N})^{N-n}
$$
and if $N$ goes to infinity
$$
\begin{aligned}
\lim_{N\to \inf}p&=\lim_{N\to \inf}\frac{N!}{(N-n)!n!N^n}(\lambda T)^n(1-\frac{\lambda T}{N})^{N-n}\\
&=\frac{(\lambda T)^n}{n!}e^{-\lambda T}
\end{aligned}
$$
And this is the Poisson distribution.    
The Exponential distribution show the probability that two independent events with probability $\lambda$ happen with time interval $t$, which means in time interval $[0,t)$, $n=0$
$$
p(t) = \lambda p_{Poisson}(0|\lambda)=\lambda e^{-\lambda t}
$$

>Beta distribution (probability distribution)
$$
\begin{aligned}
\bold X&\sim beta(x|\alpha,\beta)\\
p(x|\alpha,\beta)&=\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}
\end{aligned}
$$
where $B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$
Normally we use beta distribution to describe the dsitribution of a probability. And when $\alpha = \beta = 1$,  this is a uniform distribution. And 
$$
E(p) = \frac{\alpha}{\alpha+\beta}
$$
>Dirichlet distribution (probability vector distribution)
$$
\begin{aligned}
\bold X &\sim Dir(x_1,x_2,\dots,x_K|\alpha_1,\alpha_2,\dots,\alpha_K)\\
p(x_1,x_2,\dots ,x_K)&=\frac{1}{B(\alpha_1,\alpha_2,\dots,\alpha_K)}\prod_{i=1}^Kx_i^{\alpha_i-1}
\end{aligned}$$
where
$$
B(\alpha_1,\alpha_2,\dots,\alpha_K) = \frac{\prod_{i=1}^K\Gamma(\alpha_i)}{\Gamma(\sum_{i=1}^K\alpha_i)}
$$
## Optimization